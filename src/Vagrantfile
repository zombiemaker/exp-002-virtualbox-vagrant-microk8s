# https://www.vagrantup.com/docs/vagrantfile/

# This is just a Ruby program:  https://www.ruby-lang.org/en/documentation/
# When creating VirtualBox VMs using Vagrant, Vagrant creates a file system mount /vagrant
# that is linked to the VM host machine's directory where the vagrant program was executed.
# This enables the VM guest OS to access and share files between VMs

require 'yaml'
require 'erb'
require 'getoptlong'

# Initialize variable for optional parameters
config_file = ''

# BUG: Need to handle vagrant options
opts = GetoptLong.new(
    [ '--config-file', GetoptLong::OPTIONAL_ARGUMENT]
)

opts.each do |opt, arg|
    case opt
    when '--config-file'
        config_file = arg
    end # case opt
end # opts.each

if config_file == ''
    # Default config file is located with Vagrantfile and named config.yaml
    puts "Using default configuration file config.yaml"
    config_file = 'config.yaml'
end # if config_file

config_yaml = File.open(config_file).read
config = YAML.load( ERB.new(config_yaml).result)

# For version v0.0.1
config_api_version = config["api-version"]
if config_api_version == nil
    # For version v0.1.0
    config_api_version = config["api_version"]
end # if config_api_version

puts "configuration file #{config_file} uses api-version #{config_api_version}"


$cfg = Hash.new

# TODO:  Check config file for errors
$home_dir = File.expand_path('~')

case config_api_version
when 'v0.0.1'
    $vagrant_runtime_directory = config["vm"]["vagrant_runtime_directory"]
    $cfg[:debug] = config["activation-options"]["debug"]
    $cfg[:deploy_elb] = config["activation-options"]["deploy_elb"]
    $cfg[:deploy_edns] = config["activation-options"]["deploy_edns"]
    $cfg[:deploy_k8s_master] = config["activation-options"]["deploy_k8s_master"]
    $cfg[:deploy_k8s_worker] = config["activation-options"]["deploy_k8s_worker"]
    $cfg[:regions] = config["k8s-clusters"]["regions"]
    $cfg[:number_of_regions] = $cfg[:regions].size

    # Assuming that a cluster is needed for a separate geographic region  or availability zone
    # Each cluster needs:
    #   - External BGP router / load balancer
    #   - External DNS server
    #   - External TLS certificate authority
    #   - Kubernetes master nodes
    #   - Kubernetes worker nodes
    $cfg[:number_of_elbs_per_region] = config["k8s-clusters"]["number_of_elbs_per_region"]
    $cfg[:number_of_ednss_per_region] = config["k8s-clusters"]["number_of_ednss_per_region"]
    $cfg[:number_of_clusters_per_region] = config["k8s-clusters"]["number_of_clusters_per_region"]
    $cfg[:number_of_azs_per_region] = config["k8s-clusters"]["number_of_azs_per_region"]
    $cfg[:number_of_clusters] = $cfg[:number_of_regions] * $cfg[:number_of_clusters_per_region]
    # Master nodes and worker nodes will be in the same subnet
    # Total number of master and worker nodes cannot exceed subnet capacity (including addresses reserved for network services)
    # Distribute nodes across number of AZs
    # Each K8s cluster will use 1 IPv4 class C address range (for this design)
    # Estimate 100 pods per worker node
    $cfg[:k8s_masters_per_cluster] = config["k8s-clusters"]["master_nodes_per_cluster"]
    $cfg[:k8s_workers_per_cluster] = config["k8s-clusters"]["worker_nodes_per_cluster"]
    $cfg[:number_ip_addresses_reserved_per_cluster] = config["k8s-clusters"]["number_ip_addresses_reserved_per_cluster"]

    $cfg[:elb_vm_box] = config["elb-vm"]["vagrant-box"]
    $cfg[:elb_vm_disk_size] = "20GB"
    $cfg[:elb_vm_vb_gui] = config["elb-vm"]["virtualbox"]["gui"]
    $cfg[:elb_vm_vb_memory] = config["elb-vm"]["virtualbox"]["memory"]
    $cfg[:elb_vm_vb_cpus] = config["elb-vm"]["virtualbox"]["cpus"]

    $cfg[:edns_vm_box] = config["elb-vm"]["vagrant-box"]
    $cfg[:edns_vm_disk_size] = "20GB"
    $cfg[:edns_vm_vb_gui] = config["elb-vm"]["virtualbox"]["gui"]
    $cfg[:edns_vm_vb_memory] = config["elb-vm"]["virtualbox"]["memory"]
    $cfg[:edns_vm_vb_cpus] = config["elb-vm"]["virtualbox"]["cpus"]

    $cfg[:k8s_master_vm_box] = config["k8s-master-vm"]["vagrant-box"]
    $cfg[:k8s_master_vm_disk_size] = "20GB"
    $cfg[:k8s_master_vm_vb_gui] = config["k8s-master-vm"]["virtualbox"]["gui"]
    $cfg[:k8s_master_vm_vb_memory] = config["k8s-master-vm"]["virtualbox"]["memory"]
    $cfg[:k8s_master_vm_vb_cpus] = config["k8s-master-vm"]["virtualbox"]["cpus"]

    $cfg[:k8s_worker_vm_box] = config["k8s-worker-vm"]["vagrant-box"]
    $cfg[:k8s_worker_vm_disk_size] = "40GB"
    $cfg[:k8s_worker_vm_vb_gui] = config["k8s-worker-vm"]["virtualbox"]["gui"]
    $cfg[:k8s_worker_vm_vb_memory] = config["k8s-worker-vm"]["virtualbox"]["memory"]
    $cfg[:k8s_worker_vm_vb_cpus] = config["k8s-worker-vm"]["virtualbox"]["cpus"]
when 'v0.1.0'
else
    puts "Unsupported API version #{config_api_version}"
end # case config_api_version



Vagrant.require_version ">= 1.3.5"
Vagrant.configure("2") do |cfg|

    # Configurations for all VMs
    # Enable SSH authentication using username and password
    cfg.vm.provision "shell", inline: <<-EOF
        echo "Changing sshd configuration to allow username and password authentication"
        sudo sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config

        echo "Restarting sshd process"
        sudo systemctl restart sshd
    EOF


    # VM specific configurations
    (1..$cfg[:number_of_regions]).each do |r|

        # ELBs and DNSs are region level components
        if $cfg[:deploy_elb]
            (1..$cfg[:number_of_elbs_per_region]).each do |elb_id|
                # Cluster BGP Router / External Load Balancer (assuming 2 ELBs per cluster)
                
                cfg.vm.define("k8s-r#{r}-elb#{elb_id}") do |elb|
                    puts "Configuring k8s elb k8s-r#{r}-elb#{elb_id}" if $cfg[:debug]

                    elb_vm_name = "k8s-r#{r}-elb#{elb_id}"
                    elb_vm_hostname = "k8s-r#{r}-elb#{elb_id}"
                    elb_vm_ipv4_address = "10.#{r}.0.#{elb_id}"

                    elb.vm.box = elb_vm_box
                    elb.vm.hostname = elb_vm_hostname
                    elb.vm.network("private_network", ip: elb_vm_ipv4_address, netmask: "255.255.255.0")
                    elb.vm.disk(:disk, size: elb_vm_disk_size, primary: true)
                    elb.vm.provider("virtualbox") do |vb|
                        vb.name = elb_vm_name
                        vb.gui = $cfg[:elb_vm_vb_gui]
                        vb.memory = $cfg[:elb_vm_vb_memory]
                        vb.cpus = $cfg[:elb_vm_vb_cpus]
                    end # vb

                    elb.vm.provision "shell", inline: <<-EOF
                        echo "Configuring k8s-r#{r}-elb#{elb_id}"
                        sudo apt update && apt upgrade -y
                        sudo apt install quagga-bgpd quagga-doc -y
                    EOF

                    # TODO: delete old cluster data when creating new cluster
                    # TODO: clean up ssh known_hosts in WSL
                    # This does not clean if landscape is destroyed outside of vagrant program
                    elb.trigger.after :destroy do |trigger|
                        trigger.name = "#{elb_vm_hostname} Post-Destroy Trigger"
                        trigger.info = "Removing machine from ssh known_hosts"

                        # Remove old ssh fingerprint form previous instances
                        trigger.run = { inline: "ssh-keygen -f \"#{$home_dir}/.ssh/known_hosts\" -R \"#{elb_vm_ipv4_address}\"" }
                    end # trigger.after destroy
                end # elb config.vm.define
            end # elb_id
        end # if deploy_elb

        # Cluster External DNS Authoritative Servers
        if $cfg[:deploy_edns]
            (1..$cfg[:number_of_ednss_per_region]).each do |edns_id|
                cfg.vm.define("k8s-r#{r}-c#{c}-edns#{edns_id}") do |edns|
                    puts "Configuring k8s edns k8s-r#{r}-edns#{edns_id}" if $cfg[:debug]

                    edns_vm_name = "k8s-r#{r}-c#{c}-edns#{edns_id}"
                    edns_vm_hostname = "k8s-r#{r}-c#{c}-edns#{edns_id}"
                    edns_vm_ipv4_address = "10.#{r}.0.#{number_of_elbs_per_region + edns_id}"
                
                    edns.vm.box = config["edns-vm"]["vagrant-box"]
                    edns.vm.hostname = edns_vm_hostname
                    edns.vm.network("private_network", ip: edns_vm_ipv4_address, netmask: "255.255.255.0")
                    edns.vm.disk(:disk, size: "20GB", primary: true)
                    edns.vm.provider("virtualbox") do |vb|
                        vb.name = edns_vm_name
                        vb.gui = $cfg[:edns_vm_vb_gui]
                        vb.memory = $cfg[:edns_vm_vb_memory]
                        vb.cpus = $cfg[:edns_vm_vb_cpus]
                    end # vb

                    edns.vm.provision "shell", inline: <<-EOF
                        echo "Configuring #{edns_vm_name}"
                        sudo apt update && apt upgrade -y
                    EOF

                    # TODO: delete old cluster data when creating new cluster
                    # TODO: clean up ssh known_hosts in WSL
                    # This does not clean if landscape is destroyed outside of vagrant program
                    edns.trigger.after :destroy do |trigger|
                        trigger.name = "#{edns_vm_name} Post-Destroy Trigger"
                        trigger.info = "Removing machine from ssh known_hosts"

                        # Remove old ssh fingerprint form previous instances
                        trigger.run = { inline: "ssh-keygen -f \"#{$home_dir}/.ssh/known_hosts\" -R \"#{edns_vm_ipv4_address}\"" }
                    end # trigger.after destroy
                end # edns config.vm.define
            end # edns_id
        end # if deploy_edns


        (1..$cfg[:number_of_clusters_per_region]).each do |c|
            # Kubernetes Cluster Master Nodes
            if $cfg[:deploy_k8s_master]
                (1..$cfg[:k8s_masters_per_cluster]).each do |m|
                    # Define local variable to point cluster main master node
                    $k8s_master_m1 = nil
                    $k8s_master_m1_vm_hostname = nil
                    $k8s_master_m1_vm_ipv4_address = nil

                    cfg.vm.define("k8s-r#{r}-c#{c}-m#{m}") do |k8s_master|
                        puts "Configuring k8s master k8s-r#{r}-c#{c}-m#{m}" if $cfg[:debug]
                        
                        k8s_master_vm_name = "k8s-r#{r}-c#{c}-m#{m}"
                        k8s_master_vm_hostname = "k8s-r#{r}-c#{c}-m#{m}"
                        k8s_master_vm_ipv4_address = "10.#{r}.#{c}.#{m + $cfg[:number_ip_addresses_reserved_per_cluster]}"
                        
                        k8s_master.vm.box = config["k8s-master-vm"]["vagrant-box"]
                        k8s_master.vm.hostname = k8s_master_vm_hostname
                        
                        # IPv4 addresses allocated from start of address range after reserved addresses
                        k8s_master.vm.network("private_network", ip: k8s_master_vm_ipv4_address, netmask: "255.255.255.0")
                        k8s_master.vm.disk(:disk, size: "40GB", primary: true)
                        k8s_master.vm.provider("virtualbox") do |vb|
                            vb.name = k8s_master_vm_name
                            vb.gui = $cfg[:k8s_master_vm_vb_gui]
                            vb.memory = $cfg[:k8s_master_vm_vb_memory]
                            vb.cpus = $cfg[:k8s_master_vm_vb_cpus]  
                        end #vb

                        # First master node of a cluster is the main master node
                        if m == 1
                            $k8s_master_m1 = k8s_master
                            $k8s_master_m1_vm_hostname = k8s_master_vm_hostname
                            $k8s_master_m1_vm_ipv4_address = k8s_master_vm_ipv4_address
                        end # if m == 1
                        
                        k8s_master.vm.provision "shell", inline: <<-EOF
                            echo "=================================================="
                            echo "Configuring #{k8s_master_vm_hostname}"
                            echo "k8s_master_m1_vm_ipv4_address = #{$k8s_master_m1_vm_ipv4_address}"
                            
                            # Update and upgrade apt packages
                            echo "--------------------------------------------------"
                            echo "Updating / upgrading apt packages"
                            sudo apt-get update && apt-get upgrade -y
                            
                            # Update snap packages
                            echo "--------------------------------------------------"
                            echo "Refreshing Ubuntu snap packages"
                            sudo snap refresh
                            
                            # Install MicroK8s snap package
                            echo "--------------------------------------------------"
                            echo "Installing MicroK8s Ubuntu snap packages"
                            sudo snap install microk8s --classic

                            # Add user vagrant to user group microk8s
                            sudo usermod -a -G microk8s vagrant
                            
                            # Check if MicroK8s is running - wait until process is ready
                            echo "--------------------------------------------------"
                            echo "Checking MicroK8s status"
                            microk8s status --wait-ready
                            microk8s kubectl cluster-info
                            
                            # Install kubectl snap package
                            echo "--------------------------------------------------"
                            echo "Installing kubectl snap package"
                            sudo snap install kubectl --classic

                            # Created for each cluster's first main master - other nodes copy
                            if [ "#{k8s_master_vm_hostname}" == "#{$k8s_master_m1_vm_hostname}" ]; then
                                echo "--------------------------------------------------"
                                echo "Creating kubectl config file for cluster"

                                if [[ ! -d "#{$vagrant_runtime_directory}" ]]; then
                                    echo "Directory #{$vagrant_runtime_directory} does not exist...creating"
                                    mkdir -p #{$vagrant_runtime_directory}
                                fi
                                
                                microk8s config > #{$vagrant_runtime_directory}/kubectl-config
                                sudo sed -i 's+server: https://10.0.2.15:16443+server: https://#{$k8s_master_m1_vm_ipv4_address}:16443+g' #{$vagrant_runtime_directory}/kubectl-config
                                echo "kubectl config file #{$vagrant_runtime_directory}/kubectl-config"
                                cat #{$vagrant_runtime_directory}/kubectl-config
                            fi
                            
                            # NOTE:  if the owner of the .kube and its contents are not set correctly,
                            # MicroK8s takes A VERY LONG time to perform its operations
                            echo "--------------------------------------------------"
                            if [[ ! -d "~vagrant/.kube" ]]; then
                                echo "Directory ~vagrant/.kube does not exist...creating"
                                mkdir -p ~vagrant/.kube
                                chown -R vagrant:vagrant ~vagrant/.kube
                            fi

                            echo "Copying kubectl config for cluster to ~vagrant/.kube"
                            cp #{$vagrant_runtime_directory}/kubectl-config ~vagrant/.kube/config
                            sudo chown vagrant:vagrant ~vagrant/.kube/config
                        EOF

                        # This code is executing in an unexpected sequence.  For m > 1, this code is executed before the previous
                        # provisioning code for the VM.
                        if m != 1
                            # Generate node token on main master node                            
                            $k8s_master_m1.vm.provision "shell", inline: <<-EOF
                                echo "Generating MicroK8s new node token for #{k8s_master_vm_hostname}"
                                
                                if [[ ! -d "#{$vagrant_runtime_directory}" ]]; then
                                    echo "Directory #{$vagrant_runtime_directory} does not exist...creating"
                                    mkdir #{$vagrant_runtime_directory}
                                fi
                                
                                microk8s add-node | grep #{$k8s_master_m1_vm_ipv4_address} | tee #{$vagrant_runtime_directory}/add_k8s_node_#{k8s_master_vm_hostname}
                            EOF

                            # Join cluster on new node using generated token
                            k8s_master.vm.provision "shell", inline: <<-EOF
                                echo "Joining MicroK8s cluster using new node token"
                                bash -x #{$vagrant_runtime_directory}/add_k8s_node_#{k8s_master_vm_hostname}
                            EOF
                        end # if m != 1

                        
                        # TODO: delete old cluster data when creating new cluster
                        # TODO: clean up ssh known_hosts in WSL
                        # This does not clean if landscape is destroyed outside of vagrant program
                        k8s_master.trigger.after :destroy do |trigger|
                            trigger.name = "#{k8s_master_vm_name} Post-Destroy Trigger"
                            trigger.info = "Removing machine from ssh known_hosts"

                            # Remove old ssh fingerprint form previous instances
                            trigger.run = { inline: "ssh-keygen -f \"#{$home_dir}/.ssh/known_hosts\" -R \"#{k8s_master_vm_ipv4_address}\"" }
                        end # trigger.after destroy
                    end # k8s_master config.vm.define
                end # m
            end # if deploy_k8s_master

            # Kubernetes Cluster Worker Nodes
            if $cfg[:deploy_k8s_worker]
                (1..$cfg[:k8s_workers_per_cluster]).each do |w|
                    cfg.vm.define("k8s-r#{r}-c#{c}-w#{w}") do |k8s_worker|
                        puts "Configuring k8s worker k8s-r#{r}-c#{c}-w#{w}" if $cfg[:debug]

                        k8s_worker_vm_name = "k8s-r#{r}-c#{c}-w#{w}"
                        k8s_worker_vm_hostname = "k8s-r#{r}-c#{c}-w#{w}"
                        k8s_worker_vm_ipv4_address = "10.#{r}.#{c}.#{255-w}"

                        k8s_worker.vm.box = config["k8s-worker-vm"]["vagrant-box"]
                        k8s_worker.vm.hostname = k8s_worker_vm_hostname

                        # IPv4 addresses allocated from end of address range
                        k8s_worker.vm.network("private_network", ip: k8s_worker_vm_ipv4_address, netmask: "255.255.255.0")
                        k8s_worker.vm.disk(:disk, size: "70GB", primary: true)
                        k8s_worker.vm.provider("virtualbox") do |vb|
                            vb.name = k8s_worker_vm_name
                            vb.gui = $cfg[:k8s_worker_vm_vb_gui]
                            vb.memory = $cfg[:k8s_worker_vm_vb_memory]
                            vb.cpus = $cfg[:k8s_worker_vm_vb_cpus]         
                        end # vb

                        k8s_worker.vm.provision "shell", inline: <<-EOF
                            echo "Configuring #{k8s_worker_vm_hostname}"
                        EOF
                        
                        # TODO: delete old cluster data when creating new cluster
                        # TODO: clean up ssh known_hosts in WSL
                        # This does not clean if landscape is destroyed outside of vagrant program
                        k8s_worker.trigger.after :destroy do |trigger|
                            trigger.name = "#{k8s_worker_vm_name} Post-Destroy Trigger"
                            trigger.info = "Removing machine from ssh known_hosts"

                            # Remove old ssh fingerprint form previous instances
                            trigger.run = { inline: "ssh-keygen -f \"#{$home_dir}/.ssh/known_hosts\" -R \"#{k8s_worker_vm_ipv4_address}\"" }
                        end # trigger.after destroy
                    end # k8s_worker config.vm.define
                end # w
            end # if deploy_k8s_worker
        end # c
    end # r
end # config