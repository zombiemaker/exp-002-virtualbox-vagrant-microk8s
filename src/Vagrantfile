# https://www.vagrantup.com/docs/vagrantfile/

# This is just a Ruby program:  https://www.ruby-lang.org/en/documentation/
# When creating VirtualBox VMs using Vagrant, Vagrant creates a file system mount /vagrant
# that is linked to the VM host machine's directory where the vagrant program was executed.
# This enables the VM guest OS to access and share files between VMs

require 'yaml'
require 'erb'

config_yaml = File.open('config.yaml').read
config = YAML.load( ERB.new(config_yaml).result)

debug = config["activation-options"]["debug"]
deploy_elb = config["activation-options"]["deploy_elb"]
deploy_edns = config["activation-options"]["deploy_edns"]
deploy_k8s_master = config["activation-options"]["deploy_k8s_master"]
deploy_k8s_worker = config["activation-options"]["deploy_k8s_worker"]

$home_dir = File.expand_path('~')
$vagrant_runtime_directory = config["vm"]["vagrant_runtime_directory"]

# Assuming that a cluster is needed for a separate geographic region  or availability zone
# Each cluster needs:
#   - External BGP router / load balancer
#   - External DNS server
#   - External TLS certificate authority
#   - Kubernetes master nodes
#   - Kubernetes worker nodes

regions = config["k8s-clusters"]["regions"]
number_of_regions = regions.size
number_of_elbs_per_region = config["k8s-clusters"]["number_of_elbs_per_region"]
number_of_ednss_per_region = config["k8s-clusters"]["number_of_ednss_per_region"]
number_of_clusters_per_region = config["k8s-clusters"]["number_of_clusters_per_region"]
number_of_azs_per_region = config["k8s-clusters"]["number_of_azs_per_region"]
number_of_clusters = number_of_regions * number_of_clusters_per_region

# Master nodes and worker nodes will be in the same subnet
# Total number of master and worker nodes cannot exceed subnet capacity (including addresses reserved for network services)
# Distribute nodes across number of AZs
# Each K8s cluster will use 1 IPv4 class C address range (for this design)
# Estimate 100 pods per worker node
master_nodes_per_cluster = config["k8s-clusters"]["master_nodes_per_cluster"]
worker_nodes_per_cluster = config["k8s-clusters"]["worker_nodes_per_cluster"]
number_ip_addresses_reserved_per_cluster = config["k8s-clusters"]["number_ip_addresses_reserved_per_cluster"]

Vagrant.require_version ">= 1.3.5"
Vagrant.configure("2") do |cfg|

    # Configurations for all VMs
    # Enable SSH authentication using username and password
    cfg.vm.provision "shell", inline: <<-EOF
        echo "Changing sshd configuration to allow username and password authentication"
        sudo sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config

        echo "Restarting sshd process"
        sudo systemctl restart sshd
    EOF


    # VM specific configurations
    (1..number_of_regions).each do |r|

        # ELBs and DNSs are region level components
        if deploy_elb
            puts "Deploying external load balancer for K8s cluster"
            (1..number_of_elbs_per_region).each do |elb_id|
                # Cluster BGP Router / External Load Balancer (assuming 2 ELBs per cluster)
                
                cfg.vm.define("k8s-r#{r}-elb#{elb_id}") do |elb|
                    elb_vm_name = "k8s-r#{r}-elb#{elb_id}"
                    elb_vm_hostname = "k8s-r#{r}-elb#{elb_id}"
                    elb_vm_ipv4_address = "10.#{r}.0.#{elb_id}"
                    

                    elb.vm.box = config["elb-vm"]["vagrant-box"]
                    elb.vm.hostname = elb_vm_hostname
                    elb.vm.network("private_network", ip: elb_vm_ipv4_address, netmask: "255.255.255.0")
                    elb.vm.disk(:disk, size: "20GB", primary: true)
                    elb.vm.provider("virtualbox") do |vb|
                        vb.name = elb_vm_name
                        vb.gui = config["elb-vm"]["virtualbox"]["gui"]
                        vb.memory = config["elb-vm"]["virtualbox"]["memory"]
                        vb.cpus = config["elb-vm"]["virtualbox"]["cpus"]
                    end # vb

                    elb.vm.provision "shell", inline: <<-EOF
                        echo "Configuring k8s-r#{r}-elb#{elb_id}"
                        sudo apt update && apt upgrade -y
                        sudo apt install quagga-bgpd quagga-doc -y
                    EOF

                    # Clean up ssh client fingerprint from previous runs
                    # TODO: clean up ssh know_hosts in WSL
                    elb.trigger.before :up do |trigger|
                        trigger.run = { inline: "ssh-keygen -f \"#{$home_dir}/.ssh/known_hosts\" -R \"#{elb_vm_ipv4_address}\"" }
                    end # trigger
                end # elb config.vm.define
            end # elb_id
        end # if deploy_elb

        # Cluster External DNS Authoritative Servers
        if deploy_edns
            puts "Deploying external DNS service provider for K8s cluster"
            (1..number_of_ednss_per_region).each do |edns_id|
                cfg.vm.define("k8s-r#{r}-edns#{edns_id}") do |edns|
                    edns_vm_name = "k8s-r#{r}-edns#{edns_id}"
                    edns_vm_hostname = "k8s-r#{r}-edns#{edns_id}"
                    edns_vm_ipv4_address = "10.#{r}.0.#{number_of_elbs_per_region + edns_id}"
                
                    edns.vm.box = config["edns-vm"]["vagrant-box"]
                    edns.vm.hostname = edns_vm_hostname
                    edns.vm.network("private_network", ip: edns_vm_ipv4_address, netmask: "255.255.255.0")
                    edns.vm.disk(:disk, size: "20GB", primary: true)
                    edns.vm.provider("virtualbox") do |vb|
                        vb.name = edns_vm_name
                        vb.gui = config["edns-vm"]["virtualbox"]["gui"]
                        vb.memory = config["edns-vm"]["virtualbox"]["memory"]
                        vb.cpus = config["edns-vm"]["virtualbox"]["cpus"]            
                    end # vb

                    edns.vm.provision "shell", inline: <<-EOF
                        echo "Configuring k8s-r#{r}-edns#{edns_id}"
                        sudo apt update && apt upgrade -y
                    EOF

                    # Clean up ssh client fingerprint from previous runs
                    # TODO: clean up ssh know_hosts in WSL
                    edns.trigger.before :up do |trigger|
                        trigger.run = { inline: "ssh-keygen -f \"#{$home_dir}/.ssh/known_hosts\" -R \"#{edns_vm_ipv4_address}\"" }
                    end # trigger
                end # edns config.vm.define
            end # edns_id
        end # if deploy_edns


        (1..number_of_clusters_per_region).each do |c|
            # Kubernetes Cluster Master Nodes
            if deploy_k8s_master
                puts "Deploying K8s cluster master nodes"
                (1..master_nodes_per_cluster).each do |m|
                    # Define local variable to point cluster main master node
                    $master_node_m1 = nil
                    $master_node_m1_vm_hostname = nil
                    $master_node_m1_vm_ipv4_address = nil

                    cfg.vm.define("k8s-r#{r}-c#{c}-m#{m}") do |master_node|
                        master_node_vm_name = "k8s-r#{r}-c#{c}-m#{m}"
                        master_node_vm_hostname = "k8s-r#{r}-c#{c}-m#{m}"
                        master_node_vm_ipv4_address = "10.#{r}.#{c}.#{m + number_ip_addresses_reserved_per_cluster}"
                        
                        master_node.vm.box = config["k8s-master-vm"]["vagrant-box"]
                        master_node.vm.hostname = $master_node_vm_hostname
                        
                        # IPv4 addresses allocated from start of address range after reserved addresses
                        master_node.vm.network("private_network", ip: master_node_vm_ipv4_address, netmask: "255.255.255.0")
                        master_node.vm.disk(:disk, size: "40GB", primary: true)
                        master_node.vm.provider("virtualbox") do |vb|
                            vb.name = master_node_vm_name
                            vb.gui = config["k8s-master-vm"]["virtualbox"]["gui"]
                            vb.memory = config["k8s-master-vm"]["virtualbox"]["memory"]
                            vb.cpus = config["k8s-master-vm"]["virtualbox"]["cpus"]      
                        end #vb

                        if m == 1
                            $master_node_m1 = master_node
                            $master_node_m1_vm_hostname = master_node_vm_hostname
                            $master_node_m1_vm_ipv4_address = master_node_vm_ipv4_address
                        end # if m == 1

                        if debug
                            puts "Current master_node object id = #{master_node.object_id}"
                            puts "Current master_node.vm = #{master_node.vm}"
                            puts "master_node_m1 object id = #{$master_node_m1.object_id}"
                            puts "master_node_m1.vm = #{$master_node_m1.vm}"
                        end # if debug
                        
                        master_node.vm.provision "shell", inline: <<-EOF
                            echo "=================================================="
                            echo "Configuring #{master_node_vm_hostname}"
                            echo "master_node_m1_vm_ipv4_address = #{$master_node_m1_vm_ipv4_address}"
                            
                            # Update and upgrade apt packages
                            echo "--------------------------------------------------"
                            echo "Updating / upgrading apt packages"
                            sudo apt-get update && apt-get upgrade -y
                            
                            # Update snap packages
                            echo "--------------------------------------------------"
                            echo "Refreshing Ubuntu snap packages"
                            sudo snap refresh
                            
                            # Install MicroK8s snap package
                            echo "--------------------------------------------------"
                            echo "Installing MicroK8s Ubuntu snap packages"
                            sudo snap install microk8s --classic

                            # Add user vagrant to user group microk8s
                            sudo usermod -a -G microk8s vagrant
                            
                            # Check if MicroK8s is running - wait until process is ready
                            echo "--------------------------------------------------"
                            echo "Checking MicroK8s status"
                            microk8s status --wait-ready
                            microk8s kubectl cluster-info
                            
                            # Install kubectl snap package
                            echo "--------------------------------------------------"
                            echo "Installing kubectl snap package"
                            sudo snap install kubectl --classic

                            echo "--------------------------------------------------"
                            echo "Creating kubectl config file for cluster"

                            if [ ! -d "#{$vagrant_runtime_directory}" ]
                            then
                                echo "Directory #{$vagrant_runtime_directory} does not exist...creating"
                                mkdir #{$vagrant_runtime_directory}
                            fi
                            
                            microk8s config > #{$vagrant_runtime_directory}/kubectl-config
                            sudo sed -i 's+server: https://10.0.2.15:16443+https://10.1.1.6:16443+g' #{$vagrant_runtime_directory}/kubectl-config
                            
                            echo "--------------------------------------------------"
                            echo "Copying kubectl config for cluster to ~vagrant/.kube"
                            cp #{$vagrant_runtime_directory}/kubectl-config ~vagrant/.kube/config
                            chown vagrant:vagrant ~vagrant/.kube/config
                        EOF

                        if m != 1
                            # Generate node token on main master node                            
                            $master_node_m1.vm.provision "shell", inline: <<-EOF
                                echo "Generating MicroK8s new node token for #{master_node_vm_hostname}"
                                
                                if [ ! -d "#{$vagrant_runtime_directory}" ]
                                then
                                    echo "Directory #{$vagrant_runtime_directory} does not exist...creating"
                                    mkdir #{$vagrant_runtime_directory}
                                fi
                                
                                microk8s add-node | grep #{$master_node_m1_vm_ipv4_address} | tee #{$vagrant_runtime_directory}/add_k8s_node_#{master_node_vm_hostname}
                            EOF

                            # Join cluster on new node using generated token
                            master_node.vm.provision "shell", inline: <<-EOF
                                echo "Joining MicroK8s cluster using new node token"
                                bash -x #{$vagrant_runtime_directory}/add_k8s_node_#{master_node_vm_hostname}
                            EOF
                        end # if m != 1

                        # Clean up ssh client fingerprint from previous runs
                        # TODO: clean up ssh know_hosts in WSL
                        master_node.trigger.before :up do |trigger|
                            trigger.run = { inline: "ssh-keygen -f \"#{$home_dir}/.ssh/known_hosts\" -R \"#{master_node_vm_ipv4_address}\"" }
                        end # trigger
                    end # master_node config.vm.define
                end # m
            end # if deploy_k8s_master

            # Kubernetes Cluster Worker Nodes
            if deploy_k8s_worker
                puts "Deploying K8s cluster worker nodes"

                (1..worker_nodes_per_cluster).each do |w|
                    cfg.vm.define("k8s-r#{r}-c#{c}-w#{w}") do |worker_node|
                        worker_node_vm_name = "k8s-r#{r}-c#{c}-w#{w}"
                        worker_node_vm_hostname = "k8s-r#{r}-c#{c}-w#{w}"
                        worker_node_vm_ipv4_address = "10.#{r}.#{c}.#{255-w}"

                        worker_node.vm.box = config["k8s-worker-vm"]["vagrant-box"]
                        worker_node.vm.hostname = worker_node_vm_hostname

                        # IPv4 addresses allocated from end of address range
                        worker_node.vm.network("private_network", ip: worker_node_vm_ipv4_address, netmask: "255.255.255.0")
                        worker_node.vm.disk(:disk, size: "70GB", primary: true)
                        worker_node.vm.provider("virtualbox") do |vb|
                            vb.name = worker_node_vm_name
                            vb.gui = config["k8s-worker-vm"]["virtualbox"]["gui"]
                            vb.memory = config["k8s-worker-vm"]["virtualbox"]["memory"]
                            vb.cpus = config["k8s-worker-vm"]["virtualbox"]["cpus"]         
                        end # vb

                        worker_node.vm.provision "shell", inline: <<-EOF
                            echo "Configuring #{worker_node_vm_hostname}"
                        EOF
                        # Clean up ssh client fingerprint from previous runs
                        # TODO: clean up ssh know_hosts in WSL
                        worker_node.trigger.before :up do |trigger|
                            trigger.run = { inline: "ssh-keygen -f \"#{$home_dir}/.ssh/known_hosts\" -R \"#{worker_node_vm_ipv4_address}\"" }
                        end # trigger
                    end # worker_node config.vm.define
                end # w
            end # if deploy_k8s_worker
        end # c
    end # r
end # config